---
title: Decoupled Neural Interfaces using Synthetic Gradients
categories: notes
mathjax: true
abbrlink: 51469
date: 2020-02-18 11:04:59
tags: 
    - synthetic gradients
---

# Decoupled Neural Interfaces using Synthetic Gradients

## Abstract

神经网络的训练通常需要计算图向前推到，然后再向后传播误差更新权重。因此从某种意义上来说，网络的所有层都被锁定了，因为他们必须等待网络其他部分向前推理并向后传播才能对其进行更新。在这项工作中，我们引入了网络图的未来计算模型，来通过对模块进行解耦，从而打破这个约束。这些模型仅使用局部信息即可预测子图的结果。尤其是，当我们专注于对误差梯度进行建模：通过使用建模的合成梯度来代替真实的反向传播误差梯度，从而使得子图解耦，并且可以独立且异步的更新子图，即我们实现了解耦的神经网络接口。我们展示了前馈模型的结果，其中的每一层都是异步训练的；RNN的结果，可以预测其中一层的未来的梯度，从而延长了RNN有效建模的时间；还有分层的RNN系统的结果，在不同的时标有刻度。最后，我们证明了，除了预测梯度之外，相同的框架还可以用于预测输入，从而导致模型在向前传播和向后传播都是解耦的，等于两个独立的网络，他们可以共同学习，从而将其组成一个单独的功能网络。

<!-- more -->

## Introduction

有向神经网络中的每个层（或者模块）都可以视为一个计算步骤，转换其输入数据。这些模块通过有向边链接，从而建立了前馈图，该图定义了网络输入的数据流，通过每个模块，产生网络输出。对输出定义loss函数来产生误差并通过网络图反向传播回去以更新每个模块的权重。

这种机制导致了如下几个形式的锁定locking:

1. Forward Locking 前向锁定：在执行前向推理图中的先前节点之前，没有模块能够处理其输入数据。
2. Update Locking 更新锁定：在所有相关模块在前向图执行完毕之前，无法更新任何模块。
3. Backwards Locking 反向锁定：在所有相关模块在前馈和反向模型执行完之前，没有模块可以被更新。比如BP算法。

前向，更新，反向锁定限制了神经网络必须以顺序，同步的方式运行和更新。对于简单网络，看似良性，但是对于大型复杂不规则或异步时间尺度的多个环境中运行的网络系统就会有很大问题。

例如：分布式模型，其中模型的一部分是被许多下行的客户机共享和使用。这就意味着所有客户机必须完全执行并且将误差梯度传递会共享模型然后才能更新，这意味着这个系统训练速度由最慢的客户机决定。如果能够对当前网络的训练进行并行化，那么就可以极大地加快计算时间。

本项工作的目标是移除神经网络的更新锁定。这可以通过移除反向传播实现。为了更新模块i的权重$\theta_{i}$，我们尽可能的近似了反向传播的函数：

$$
\begin{aligned} \frac{\partial L}{\partial \theta_{i}} &=f_{\text {Bprop }}\left(\left(h_{i}, x_{i}, y_{i}, \theta_{i}\right), \ldots\right) \frac{\partial h_{i}}{\partial \theta_{i}} \\ & \simeq \hat{f}_{\text {Bprop }}\left(h_{i}\right) \frac{\partial h_{i}}{\partial \theta_{i}} \end{aligned}
$$

其中h表示激活层，x是输入，y是监督（标签），L是总体损失。这使得其更新的依赖全部落在了h上，也就是模块i的局部信息。

该方法的前提是允许神经网络模块进行交互并且在没有更新锁定的情况下进行训练。在这里作者将传统的神经网络接口（神经网络中两个模块连接）替换为解耦神经接口（DNI）。简单来说，就是当一个网络层向另一个层传递激活值的时候，会有一个相关的模型，对该激活值产生一个预测的误差梯度。该预测的梯度仅仅是对该激活值的一个函数，而不依赖与其他事件状态或者损失。然后发送的那个网络层可以立即使用这个合成梯度来进行更新。通过移除更新和反向锁定，我们可以无需同步就能训练网络。我们还展示了初步的结果，将这一思想扩展并且也移除了前向锁定，从而使得网络的模块也可以在没有同步前向通过的情况下进行训练。当将其应用于RNN时，我们表明使用合成梯度可以使RNN建模的时间范围远大于BPTT的限制。我们还表明，使用合成梯度对两个在不同时间范围内的RNN进行解耦可以大大提高训练效果。

## Decoupled Neural Interfaces

首先描述high-level的通信协议，该协议用于允许异步学习agents进行通信。

正如图1所示， Sender A 发送信息（激活值）ha给Receiver B。简单来说可以理解为A是前面的layer，B是后一个layer。B有一个utility是MB用于处理信号ha，来预测反馈。误差信号：$\hat{\delta}_{A}=M_{B}\left(h_{A}, s_{B}, c\right)$ 其中，ha是信息（激活），B的状态SB，和一些其他的潜在信息c，例如标签或context。A可以立即利用该误差信号进行更新。B也可以及时完全评估真是的sigmaA，因此，B的utility模型可以被更新用于fit真是的utility，减少真实误差和合成误差的差异。

该协议允许A以A和B更新解耦的方式向B发送消息– A不必等待B评估真实效用就可以对其进行更新–并且A仍然可以学习发送消息到B。

我们可以将该协议用于网络的通信过程，从而产生所谓的去耦神经接口（DNI）。对于神经网络，反馈误差信号sigma_hat_A可以采用不同的形式，例如：梯度可以用作与反向传播一起使用的误差信号，目标信息可以用作与目标传播一起使用的误差信号，甚至可以用作结合到强化学习框架中的值。本文专注在通过反向传播和基于梯度更新的可微网络上。因此，专注于产生的误差梯度作为反馈sigma_hat_a，称为合成梯度。

Notation。我们将step为i的函数定义为fi，然后从stepi到stepj之间的函数的组成称为Fij。Layer i的损失定义为Li。

### Synthetic Gradient for Feed-Forward Networks

考虑前馈网络的DNI形式，N个layers fi，每个接受输入hi-1，并产生输出hi=fi（hi-1），其中h0=x是输入数据。整个网络的前馈图可以表示为F1N。见图3（a）

![20200218134533.png](http://cdn.ereebay.me/hexo/20200218134533.png)

定义网络输出的损失函数为L=LN。每个网络层fi有参数thetai，通过梯度更新的规则用来最小化L(hN）

$$
\theta_{i} \leftarrow \theta_{i}-\alpha \delta_{i} \frac{\partial h_{i}}{\partial \theta_{i}} ; \quad \delta_{i}=\frac{\partial L}{\partial h_{i}}
$$

α是学习率，对hi的求导是通过反向传播。对于sigmai的依赖意味着网络层i的更新必须在剩余部分更新后才行。即Fi+1N都已经执行了前馈和反馈阶段。因此，网络层i更新锁定在了Fi+1N。

为了移除更新锁定，采用前面提到的通信协议。网络层i发送信息hi给后一个网络层，其有通信模型Mi+1，来产生合成误差梯度sigma_hat_i = Mi+1(hi)。如图所示：

![20200218135141.png](http://cdn.ereebay.me/hexo/20200218135141.png)

可以立即更新网络层i，以及F1i之间的其他网络层

$$
\theta_{n} \leftarrow \theta_{n}-\alpha \hat{\delta}_{i} \frac{\partial h_{i}}{\partial \theta_{n}}, n \in\{1, \ldots, i\}
$$

为了训练合成梯度模型Mi+1的参数，我们通过等待真实的误差梯度sigmai计算完成后，计算两者的mse误差。

此外，对于前馈网络，我们可以使用合成梯度作为通信反馈来解耦网络中的每个层。如图所示：

![20200218142500.png](http://cdn.ereebay.me/hexo/20200218142500.png)

这个机制的完整执行过程：

![20200218150932.png](http://cdn.ereebay.me/hexo/20200218150932.png)

在这种情况下，由于目标误差梯度sigmai是通过层i+1反向传播sigma_hat_i+1产生的。sigmai并不是真实的误差梯度，而是从后面的合成梯度模型获得的估计值。令人惊讶的是，这不会导致误差更加严重，而且即使在很多层中，也能保持稳定的学习，如后面实验所示。

此外，如果能够加上一些监督或者context c来计算合成梯度。

$$
\hat{\delta}_{i} = {M_{i+1}\left(h_{i}, c\right) }
$$

此过程允许在执行该层的前向传递后立即对其进行更新。这为以异步方式训练网络的子部分或层铺平了道路。

## Experiment

### Feed-Forward Networks

将DNI应用于前馈网络，以允许异步或者零散的单层训练，这在分布式训练中有可能出现。

正如前面解释的，通过引入合成梯度使得网络层解耦，可以使得网络层之间彼此通信而不会收到更新锁定的影响。

**Asynchronous Updates** 异步更新：为了演示通过DNI来实现的解耦层所带来的提升，我们在mnist的四层全连接网络上做了实验，其中，每层的反向传递和更新都是以概率pupdate随机顺序发生的。（即仅在前向传播了pudate的时间，网络层才会更新。）这将完全破坏反向传播，例如，第一层将仅接受概率pudate3的误差梯度更新，即使这样，系统也将被约束为同步。然而，通过DNI可以弥补每层之间的通信鸿沟，层更新的随机性不会影响下面的层，因为使用的是合成梯度。 我们使用了0到1之间均匀采样不同的pupdate的值。有标签和没标签的DNI如图7所示。

![20200218170640.png](http://cdn.ereebay.me/hexo/20200218170640.png)

使用pupdate=0.2时，网络仍然可以训练得到2%的精度。令人难以置信的是，当DNI是以数据标签为条件时（如果以分布式方式进行训练，这是一个合理的假设），网络将以5%的更新概率完美进行训练，尽管速度变慢了。

### Complete Unlock

通过消除前向锁定完全使得前馈网络完全异步，这种情况下，每个网络层都会有一个合成的梯度模型，同时也有一个合成的输入模型 用于预测输入数据。如下图所示：

![20200218175611.png](http://cdn.ereebay.me/hexo/20200218175611.png)

每个网络层都可以依靠合成梯度和合成的输入模型进行单独训练。下图是实验结果。

![20200218175713.png](http://cdn.ereebay.me/hexo/20200218175713.png)

实验表明，在这种情况下，模型可以进行完全异步的独立训练，但是要达到2%的误差率精度，训练时间要更长一些。

## Discussion & Conclusion

本文主要介绍了一种使用合成梯度的DNI方法，可以将网络层之间的通信进行解耦，让他们进行独立更新。也证明了该方法可以完全分离网络所有的层，从而使他们可以完全异步，非序，偶发的训练。

值得注意的是，尽管本文介绍并展示了DNI和合成梯度的有效性的经验依据，但是czarnecki等人的工作更深入研究了其理论理解，证实了收敛性。
